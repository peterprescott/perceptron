{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP527 Data Mining & Visualization: Text Classification Using Binary Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student 201442927. University of Liverpool.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Explain the Perceptron algorithm for the binary classification case, providing its pseudo code. (20 marks)\n",
    "\n",
    "(2) Prove that for a linearly separable dataset, perceptron algorithm will converge. (10 marks)\n",
    "\n",
    "(3) Implement a binary perceptron. (20 marks)\n",
    "\n",
    "(4) Use the binary perceptron to train classifiers to discriminate between (a) class 1 and class 2,\n",
    "(b) class 2 and class 3 and (c) class 1 and class 3. Report the train and test classification\n",
    "accuracies for each of the three classifiers after 20 iterations. Which pair of classes is most\n",
    "difficult to separate? (20 marks)\n",
    "\n",
    "(5) For the classifier (a) implemented in part (3) above, which feature is the most discriminative? (5 marks)\n",
    "\n",
    "(6) Extend the binary perceptron that you implemented in part (2) above to perform multi-class\n",
    "classification using the 1-vs-rest approach. Report the train and test classification accuracies\n",
    "for each of the three classes after training for 20 iterations. (15 marks),\n",
    "\n",
    "(7) Add an $ \\ell_{2} $ regularisation term to your multi-class classifier implemented in question (5). Set\n",
    "the regularisation coefficient to 0.01, 0.1, 1.0, 10.0, 100.0 and compare the train and test\n",
    "classification accuracy for each of the three classes. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Explain...\n",
    ">Explain the Perceptron algorithm for the binary classification case, providing its pseudo code. (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Perceptron Algorithm*, published by Frank Rosenblatt in 1958, is inspired by the idea of a biological neuron which is sensitive to a number of stimuli and is deterministically activated when the effect of those combined stimuli exceeds some activation threshold.\n",
    "\n",
    "Mathematically, we model this as the dot product of a vector of quantified stimuli $\\mathbf{x}$ and a vector of weighted sensitivities $\\mathbf{w}$, plus a bias term $ b $.\n",
    "\n",
    "We iterate through our training dataset of vectors with labels ( $\\mathbf{x}_{i} \\in \\Bbb{R}^{N}, y_{i} \\in $ {-1, 1} ), and whenever we get the wrong result, we adjust our weight vector accordingly: if our result was negative when it should have been positive, we *add* the incorrectly classified vector to the weight vector; if it was positive when it should have been negative then we *subtract* the incorrectly classified vector. \n",
    "\n",
    "We also adjust our bias term, by adding the label $y_{i} \\in $ {-1, 1}.\n",
    "\n",
    "We repeat until the Perceptron is able to correctly classify every element in the training set, or when some specified maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pseudo-code given by Daume III (2017:43), we have:\n",
    "\n",
    "---\n",
    "PerceptronTrain($\\mathbf{D}$, *MaxIter*)\n",
    "\n",
    "$w_{d} \\leftarrow 0$, for all d = 1 ... D\n",
    "$b \\leftarrow 0$\n",
    "\n",
    "**for** *iter* = 1 $...$ *MaxIter* **do**  \n",
    "$...$ **for all** ($\\mathbf{x}, y \\in \\mathbf{D})$ **do**  \n",
    "$...$ $...$ $a \\leftarrow \\sum_{d=1}^D w_{d} x_{d} + b$  \n",
    "$...$ $...$ **if** $ya \\le 0$ **then**  \n",
    "$...$ $...$ $...$ $w_{d} \\leftarrow w_{d} + yx_{d}$, for all $d = 1 ... D$  \n",
    "$...$ $...$ $...$ $ b \\leftarrow b + y$  \n",
    "$...$ $...$ **end if**  \n",
    "$...$ **end for**  \n",
    "**end for**  \n",
    "**return** $w_{0}, w_{1}, ..., w_{D}, b  $\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perceptron(dataset, max_iterations):\n",
    "    \n",
    "    w = {}\n",
    "    w[0] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Prove...\n",
    "> Prove that for a linearly separable dataset, the perceptron algorithm will converge. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a hyperplane, and show that any hyperplane $\\in \\Bbb{R}^N$ not intersecting the origin can be mapped to a hyperplane $\\in \\Bbb{R}^{N+1}$ which does intersect the origin; we then give define *linearly separable* in terms of a hyperplane; and we define the *perceptron algorithm* for an origin-intersecting hyperplane. We then prove the convergence of the perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 1: A Hyperplane.** A *hyperplane* $ H^{N-1} $ is an (N - 1) dimensional subspace of an N dimensional space, defined by a normal $ \\mathbf{n} \\in \\Bbb{R}^{N} $, and some constant $c \\in \\Bbb{R} $, such that $ H^{N-1}  = \\{  \\mathbf{x} \\in \\Bbb{R}^{N} : \\mathbf{x} \\cdot \\mathbf{n} = c \\in \\Bbb{R} \\}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 1**. We note that any hyperplane of dimension (N-1)\n",
    "can be projected to an origin-intersecting hyperplane \n",
    "$ H_{0}^{N} $ of dimension N (within an N+1 dimensional space $ \\Bbb{R}^{N+1} $), \n",
    "by mapping $f(x_{1}, \\dots, x_{N}) \\to (1, x_{1}, \\dots, x_{N})$, \n",
    "and $g(n_{1}, \\dots, n_{N}) \\to (-c, n_{1}, \\dots, n_{N})$ \n",
    "so that if we say $f(\\mathbf{x}) = \\mathbf{x}^\\prime$ \n",
    "and $g(\\mathbf{n}) = \\mathbf{n}^\\prime$ \n",
    "we have $\\mathbf{x}^\\prime \\cdot \\mathbf{n}^\\prime = -c + \\mathbf{x} \\cdot \\mathbf{n} = 0$\n",
    "\n",
    "Therefore we can describe $ H^{N}  = \\{  \\mathbf{x}^\\prime \\in \\Bbb{R}^{N+1} : \\mathbf{x}^\\prime \\cdot \\mathbf{n}^\\prime = 0 \\in \\Bbb{R}  \\}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 2.** A vectorised dataset in an N-dimensional attribute-space is *linearly separable* if there exists a hyperplane $ H $ such that all the points to one side of the hyperplane are of one category, and all the points to the other side are of the other. \n",
    "\n",
    "Thus, given labels $ y_{i} \\in \\{ 1, -1 \\} $ for each datapoint $ \\mathbf{x}_{i} \\in \\Bbb{R}^{N} $, $ \\exists \\mathbf{n} $ such that $\\forall \\mathbf{x}_{i}, \\\\ y_{i}(\\mathbf{x}_{i} \\cdot \\mathbf{n}) - c > 0 $.  \n",
    "If our hyperplane intersects the origin, then c = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 3: The Perceptron Algorithm.** \n",
    "\n",
    "Since Lemma 1 allows us to assume without loss of generality that our dataset is divided by an origin-intersecting hyperplane, we express the algorithm given that assumption.\n",
    "\n",
    "$ k \\leftarrow 1; \\mathbf{w}^{0} \\leftarrow \\mathbf{0} $.  \n",
    "While $\\exists i \\in \\{1,2,\\dots, n\\}$ such that $y_{i}(\\mathbf{w}^{k} \\cdot \\mathbf{x}_{i}) \\le 0$:  \n",
    "$\\dots$Find $j \\in \\{1,2,\\dots,n\\}$ such that $y_{j}(\\mathbf{w}^{k} \\cdot \\mathbf{x}_{j}) \\le 0$.  \n",
    "$\\dots \\mathbf{w}^{k+1} \\leftarrow \\mathbf{w}^{k} + y_{j}\\mathbf{x}_{j}$.  \n",
    "$\\dots k \\leftarrow k + 1$.  \n",
    "Return $\\mathbf{w}^{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**. *For a linearly separable dataset, the Perceptron Algorithm will converge.*\n",
    "\n",
    "**Proof**.\n",
    "\n",
    "Lemma 1 means that we can assume without loss of generality that our dataset is divided by an origin-intersecting hyperplane.\n",
    "\n",
    "In such a case it is simple to see that we can choose a normal $\\mathbf{n}$ for our hyperplane such that $\\|\\mathbf{n}\\| = 1$.\n",
    "\n",
    "Given a finite dataset, we must have some point lying closest to the separating hyperplane, such that it minimizes $$ y_{i}(\\mathbf{x}_{i} \\cdot \\mathbf{n} ) = 2 \\epsilon >  \\epsilon \\in \\Bbb{R} \\tag{1} $$\n",
    "\n",
    "We must also have some point furthest from the origin, such that it maximizes $$ \\| \\mathbf{x}_{i} \\| = \\frac{1}{2} R < R \\in \\Bbb{R} \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write $\\mathbf{w}^k$ for the k-th iteration of $\\mathbf{w}$.\n",
    "\n",
    "Then the definition of the Perceptron Algorithm, plus (1) gives us$$\n",
    "\\begin{align}\n",
    "\\mathbf{w}^{k+1} \\cdot \\mathbf{n} &= (\\mathbf{w}^k + y_{j}\\mathbf{x}_{j}) \\cdot \\mathbf{n}  \n",
    "\\\\ &= \\mathbf{w}^k \\cdot \\mathbf{n} + y_{j}(\\mathbf{x}_j \\cdot \\mathbf{n})\n",
    "\\\\ &> \\mathbf{w}^k \\cdot \\mathbf{n} + \\epsilon \\tag{3}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $$ \\mathbf{w}^k  \\cdot \\mathbf{n} > k\\epsilon \\tag{4}$$ \n",
    "for k = 1, then it does $\\forall k \\in \\Bbb{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since (3) + (4)   $$\\begin{align} \\implies \\mathbf{w}^{k+1} \\cdot \\mathbf{n} &> \\mathbf{w}^k \\cdot \\mathbf{n} + \\epsilon \\\\ &> k\\epsilon + \\epsilon = (k+1)\\epsilon \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and from Definition 3 we know that $$\\mathbf{w}_0 = \\mathbf{0}.$$\n",
    "so substituting k=0 into (3), with (4) gives us $$\\mathbf{w}^1 > \\mathbf{0} + \\epsilon$$ and thus proving (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have chosen $\\|\\mathbf{n}\\| = 1 $ (0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we know that $$\\|\\mathbf{x} \\| \\| \\mathbf{y} \\| \\ge \\mathbf{x}  \\cdot \\mathbf{y} \\tag{by Cauchy-Schwarz} $$ we therefore have $$\\begin{align} \\|\\mathbf{w}^k \\| = \\|\\mathbf{w}^k \\| \\| \\mathbf{n} \\| &\\ge \\mathbf{w}^k  \\cdot \\mathbf{n}\\\\ &> k\\epsilon \\tag{5} \\end{align}$$\n",
    "which gives us a lower bound on $\\|\\mathbf{w}\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an upper bound, we know from Definition 3 that $ \\mathbf{w}^{k+1} = \\mathbf{w}^{k} + y_{j}\\mathbf{x}_{j}$ and so \n",
    "$$\\begin{align}\\|\\mathbf{w}^{k+1}\\|^2 &= \\|\\mathbf{w}^{k} + y_{j}\\mathbf{x}_{j}\\|^2 \n",
    "\\\\ &=  \\|\\mathbf{w}^{k}\\|^2 + \\|\\mathbf{x}_{j}\\|^2 + 2y_{j}(\\mathbf{w}^k \\cdot \\mathbf{x}_j)\n",
    "\\\\ & \\le \\|\\mathbf{w}^{k}\\|^2 + \\|\\mathbf{x}_{j}\\|^2 \\tag{6}\n",
    "\\end{align}$$\n",
    "since we only update $\\mathbf{w}^k$ when it misclassifies $\\mathbf{x}$ and so $$ y_{j}(\\mathbf{w}^k \\cdot \\mathbf{x}_j) \\le 0 \\tag{Definition 3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting together (6) and (2) gives us $$ \\|\\mathbf{w}^{k+1}\\|^2 \\le \\|\\mathbf{w}^{k}\\|^2 + R^2 \\tag{7}$$\n",
    "and so by induction $$ \\|\\mathbf{w}^{k+1}\\|^2 \\le kR^2 \\tag{8}$$\n",
    "$\\forall k \\in \\Bbb{N}$ since if true for any $k \\in \\Bbb{N}$  \n",
    "then (8) + (7) $\\implies \\|\\mathbf{w}^{k+1}\\|^2 \\le  (k-1)R^2 + R^2 = kR^2 $\n",
    "and Def.3 $\\implies \\mathbf{x}^0 = \\mathbf{0} \\le R^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting together (8) and (5) we then have $$\\begin{align} kR^2 &\\ge \\|\\mathbf{w}^{k+1}\\|^2 \\\\ &> ((k+1)\\epsilon)^2 \\\\&\\ge (k\\epsilon)^2 \\tag{9} \\\\&\\ge k^2\\epsilon^2 \\tag{10}\\end{align}$$\n",
    "\n",
    "(9) is true, since $\\epsilon \\ge 0$ and $k \\in \\Bbb{N}$.\n",
    "\n",
    "From (10), we then have a limit on k: $$ k \\le \\frac{R^2}{\\epsilon^2} $$ and we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Implement...\n",
    "> Implement a binary perceptron. (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vector():\n",
    "    \"\"\"Define vector and vector-functions without using NumPy.\"\"\"\n",
    "    \n",
    "    def __init__(self, list_of_floats):\n",
    "        self._ = list_of_floats\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self._)\n",
    "    \n",
    "    def checks(self, vector):\n",
    "        try:\n",
    "            v1 = self._\n",
    "            v2 = vector._\n",
    "        except:\n",
    "            print(f'ERROR: {vector} must be of class \"vector\".')\n",
    "            return 'error'\n",
    "\n",
    "        if len(v2) != len(self._):\n",
    "            print('ERROR: Both vectors must be of same length.')\n",
    "            return 'error'   \n",
    "    \n",
    "    def dot(self, vector):\n",
    "        if self.checks(vector) == 'error':\n",
    "            return\n",
    "                \n",
    "        v1, v2 = self._ , vector._\n",
    "        dot_product = 0.0\n",
    "        for i, _ in enumerate(self._):\n",
    "            dot_product += v1[i] * v2[i]\n",
    "        return dot_product\n",
    "    \n",
    "    def norm(self):\n",
    "        return math.sqrt(self.dot(self))\n",
    "    \n",
    "    def cosine_similarity(self,vector):\n",
    "        cos_theta = self.dot(vector) / (self.norm() * vector.norm())\n",
    "        return cos_theta\n",
    "        \n",
    "    def angle(self,vector):\n",
    "        theta = math.acos(self.cosine_similarity(vector))\n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        v1 (np.array): first vector\n",
    "        v2 (np.array): second vector\"\"\"\n",
    "    \n",
    "    score = numpy.dot(v1,v2) / (numpy.linalg.norm(v1) * numpy.linalg.norm(v2))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, with_print=False):\n",
    "    \"\"\"\n",
    "    Read in labelled data from file.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Name of file in local directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # open the file\n",
    "    with open(filename,'r') as f:\n",
    "        file_data = f.read()\n",
    "\n",
    "    # split lines\n",
    "    split_data = file_data.split('\\n')\n",
    "\n",
    "    # creat dict to store data\n",
    "    data = []\n",
    "    for i, datum in enumerate(split_data):\n",
    "        try:\n",
    "            # split the data-vector from the class-label\n",
    "            split = datum.split(',class-')\n",
    "            class_name = split[1]\n",
    "            \n",
    "            # split the elements of the data-vector\n",
    "            list_of_strings = split[0].split(',')\n",
    "            list_vector = [1]\n",
    "            \n",
    "            # convert the elements of the data-vector...\n",
    "            # ... from text strings to floating-point numbers\n",
    "            for string in list_of_strings:\n",
    "                element = float(string)\n",
    "                list_vector.append(element)\n",
    "            \n",
    "            # convert the list of floats to a numpy array vector\n",
    "            vector = np.array(list_vector)\n",
    "            \n",
    "            # load the label and vector into the data dict\n",
    "            data.append((class_name, vector))\n",
    "            \n",
    "            if with_print==True:\n",
    "                print(f'Extracted \"{datum}\" to \"{data[i]}\".')\n",
    "        except IndexError:\n",
    "            if with_print==True:\n",
    "                print(f'Could not split \"{datum}\".\\nProbably this was the end of the file.')\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data('test.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_data('train.data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5a90e1dcf6d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_perceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_updates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \"\"\"Train Perceptron on given training dataset.\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtraining_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def train_perceptron(positive_label='1', negative_label='2', max_iterations=1000, max_updates=10000, training_dataset=train):\n",
    "    \"\"\"Train Perceptron on given training dataset.\n",
    "    \n",
    "    Args:\n",
    "        training_dataset(dict): Dict with integer keys containing tuples \n",
    "                            with labels and np.array data vectors.\"\"\"\n",
    "    \n",
    "    # load training_dataset\n",
    "    data = training_dataset\n",
    "    \n",
    "    # find length of dataset vectors\n",
    "    vector_length = len(data[0][1])\n",
    "    \n",
    "    # \n",
    "    w = {}\n",
    "        \n",
    "    # initialize weight_vector\n",
    "    w[0] = np.zeros(vector_length)\n",
    "    \n",
    "    # label test data\n",
    "    y = {}\n",
    "    for i, datum in enumerate(data):\n",
    "        class_name = datum[0]\n",
    "        if class_name == positive_label:\n",
    "            y[i] = 1\n",
    "        elif class_name == negative_label:\n",
    "            y[i] = -1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    \n",
    "    k = 0\n",
    "    count_updates = {}\n",
    "    for n in range(0, max_iterations):\n",
    "        \n",
    "        for i, datum in enumerate(data):\n",
    "        \n",
    "            x = datum[1]\n",
    "            \n",
    "            if y[i]*(w[k].dot(x)) <= 0:\n",
    "                # ie. if incorrectly classified\n",
    "                w[k+1] = w[k] + y[i]*x\n",
    "                k = k+1\n",
    "        count_updates[n] = k        \n",
    "        if n>0 and count_updates[n] == count_updates[n-1]:\n",
    "            break\n",
    "        if k == max_updates:\n",
    "            break\n",
    "            \n",
    "    return {'weights':w[k], \n",
    "            'positive_label':positive_label, \n",
    "            'negative_label':negative_label,\n",
    "            'updates':k,\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_perceptron('2','3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perceptron(trained_perceptron, test_dataset=test):\n",
    "    \n",
    "    w = trained_perceptron['weights']\n",
    "    \n",
    "    # label test dataset\n",
    "    y = {}\n",
    "    for i, datum in enumerate(test_dataset):\n",
    "        class_name = datum[0]\n",
    "        if class_name == trained_perceptron['positive_label']:\n",
    "            y[i] = 1\n",
    "        elif class_name == trained_perceptron['negative_label']:\n",
    "            y[i] = -1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    \n",
    "    results = []\n",
    "    for i, datum in enumerate(test_dataset):\n",
    "        x = datum[1]\n",
    "        if y[i] == 0:\n",
    "            pass\n",
    "        elif y[i] * w.dot(x) > 0:\n",
    "            results.append('right')\n",
    "        else:\n",
    "#             print(w)\n",
    "#             print(datum)\n",
    "#             print(y[i]*w.dot(x))\n",
    "            results.append('wrong')\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'wrong',\n",
       " 'right',\n",
       " 'wrong',\n",
       " 'right',\n",
       " 'wrong',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'wrong',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'wrong',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right',\n",
       " 'right']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_perceptron(train_perceptron('2','3'), train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Train...\n",
    "> Use the binary perceptron to train classifiers to discriminate between (a) class 1 and class 2, (b) class 2 and class 3 and (c) class 1 and class 3. Report the train and test classification accuracies for each of the three classifiers after 20 iterations. Which pair of classes is most difficult to separate? (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Which...\n",
    "> For the classifier (a) implemented in part (3) above, which feature is the most discriminative? (5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Extend...\n",
    "> Extend the binary perceptron that you implemented in part (2) above to perform multi-class classification using the 1-vs-rest approach. Report the train and test classification accuracies for each of the three classes after training for 20 iterations. (15 marks),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Regularise...\n",
    "> Add an $\\ell_{2}$ regularisation term to your multi-class classifier implemented in question (5). Set the regularisation coefficient to 0.01, 0.1, 1.0, 10.0, 100.0 and compare the train and test classification accuracy for each of the three classes. (10 marks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
