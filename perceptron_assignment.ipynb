{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP527 Data Mining & Visualization: Text Classification Using Binary Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student 201442927. University of Liverpool.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Explain the Perceptron algorithm for the binary classification case, providing its pseudo code. (20 marks)\n",
    "\n",
    "(2) Prove that for a linearly separable dataset, perceptron algorithm will converge. (10 marks)\n",
    "\n",
    "(3) Implement a binary perceptron. (20 marks)\n",
    "\n",
    "(4) Use the binary perceptron to train classifiers to discriminate between (a) class 1 and class 2,\n",
    "(b) class 2 and class 3 and (c) class 1 and class 3. Report the train and test classification\n",
    "accuracies for each of the three classifiers after 20 iterations. Which pair of classes is most\n",
    "difficult to separate? (20 marks)\n",
    "\n",
    "(5) For the classifier (a) implemented in part (3) above, which feature is the most discriminative? (5 marks)\n",
    "\n",
    "(6) Extend the binary perceptron that you implemented in part (2) above to perform multi-class\n",
    "classification using the 1-vs-rest approach. Report the train and test classification accuracies\n",
    "for each of the three classes after training for 20 iterations. (15 marks),\n",
    "\n",
    "(7) Add an $ \\ell_{2} $ regularisation term to your multi-class classifier implemented in question (5). Set\n",
    "the regularisation coefficient to 0.01, 0.1, 1.0, 10.0, 100.0 and compare the train and test\n",
    "classification accuracy for each of the three classes. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (1) \n",
    ">Explain the Perceptron algorithm for the binary classification case, providing its pseudo code. (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Perceptron Algorithm*, published by Frank Rosenblatt in 1958, is inspired by the idea of a biological neuron which is sensitive to a number of stimuli and is deterministically activated when the effect of those combined stimuli exceeds some activation threshold.\n",
    "\n",
    "Mathematically, we model this as the dot product of a vector of quantified stimuli $\\mathbf{x}$ and a vector of weighted sensitivities $\\mathbf{w}$, plus a bias term $ b $.\n",
    "\n",
    "We iterate through our training dataset of vectors with labels ( $\\mathbf{x}_{i} \\in \\Bbb{R}^{N}, y_{i} \\in $ {-1, 1} ), and whenever we get the wrong result, we adjust our weight vector accordingly: if our result was negative when it should have been positive, we *add* the incorrectly classified vector to the weight vector; if it was positive when it should have been negative then we *subtract* the incorrectly classified vector. \n",
    "\n",
    "We also adjust our bias term, by adding the label $y_{i} \\in $ {-1, 1}.\n",
    "\n",
    "We repeat until the Perceptron is able to correctly classify every element in the training set, or when some specified maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pseudo-code given by Daume III (2017:43), we have:\n",
    "\n",
    "---\n",
    "PerceptronTrain($\\mathbf{D}$, *MaxIter*)\n",
    "\n",
    "$w_{d} \\leftarrow 0$, for all d = 1 ... D\n",
    "$b \\leftarrow 0$\n",
    "\n",
    "**for** *iter* = 1 $...$ *MaxIter* **do**  \n",
    "$...$ **for all** ($\\mathbf{x}, y \\in \\mathbf{D})$ **do**  \n",
    "$...$ $...$ $a \\leftarrow \\sum_{d=1}^D w_{d} x_{d} + b$  \n",
    "$...$ $...$ **if** $ya \\le 0$ **then**  \n",
    "$...$ $...$ $...$ $w_{d} \\leftarrow w_{d} + yx_{d}$, for all $d = 1 ... D$  \n",
    "$...$ $...$ $...$ $ b \\leftarrow b + y$  \n",
    "$...$ $...$ **end if**  \n",
    "$...$ **end for**  \n",
    "**end for**  \n",
    "**return** $w_{0}, w_{1}, ..., w_{D}, b  $\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more simply, as given by Kalyanakrishnan (2017: 3) for the case where the data has been transformed to be separated by an origin-intersecting hyperplane:\n",
    "\n",
    "---\n",
    "$ k \\leftarrow 1; \\mathbf{w}^{0} \\leftarrow \\mathbf{0} $.  \n",
    "While there exists $i \\in \\{1,2,\\dots, n\\}$ such that $y_{i}(\\mathbf{w}^{k} \\cdot \\mathbf{x}_{i}) \\le 0$:  \n",
    "$\\dots$Pick an arbitrary $j \\in \\{1,2,\\dots,n\\}$ such that $y_{j}(\\mathbf{w}^{k} \\cdot \\mathbf{x}_{j}) \\le 0$.  \n",
    "$\\dots \\mathbf{w}^{k+1} \\leftarrow \\mathbf{w}^{k} + y_{j}x_{j}$.  \n",
    "$\\dots k \\leftarrow k + 1$.  \n",
    "Return $\\mathbf{w}^{k}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (2) \n",
    "> Prove that for a linearly separable dataset, the perceptron algorithm will converge. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 1: A Hyperplane.** A *hyperplane* $ H^{N-1} $ is an (N - 1) dimensional subspace of an N dimensional space, defined by a normal $ \\mathbf{n} \\in \\Bbb{R}^{N} $, and some constant $c \\in \\Bbb{R} $, such that $ H^{N-1}  = \\{  \\mathbf{x} \\in \\Bbb{R}^{N} : \\mathbf{x} \\cdot \\mathbf{n} = c \\in \\Bbb{R} \\}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 1**. We note that any hyperplane of dimension (N-1)\n",
    "can be projected to an origin-intersecting hyperplane \n",
    "$ H_{0}^{N} $ of dimension N (within an N+1 dimensional space $ \\Bbb{R}^{N+1} $), \n",
    "by mapping $f(x_{1}, \\dots, x_{N}) \\to (1, x_{1}, \\dots, x_{N})$, \n",
    "and $g(n_{1}, \\dots, n_{N}) \\to (-c, n_{1}, \\dots, n_{N})$ \n",
    "so that if we say $f(\\mathbf{x}) = \\mathbf{x}^\\prime$ \n",
    "and $g(\\mathbf{n}) = \\mathbf{n}^\\prime$ \n",
    "we have $\\mathbf{x}^\\prime \\cdot \\mathbf{n}^\\prime = -c + \\mathbf{x} \\cdot \\mathbf{n} = 0$\n",
    "\n",
    "Therefore we can describe $ H^{N}  = \\{  \\mathbf{x}^\\prime \\in \\Bbb{R}^{N+1} : \\mathbf{x}^\\prime \\cdot \\mathbf{n}^\\prime = 0 \\in \\Bbb{R}  \\}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 2.** A vectorised dataset in an N-dimensional attribute-space is *linearly separable* if there exists a hyperplane $ H $ such that all the points to one side of the hyperplane are of one category, and all the points to the other side are of the other. \n",
    "\n",
    "Thus, given labels $ y_{i} \\in \\{ 1, -1 \\} $ for each datapoint $ \\mathbf{x}_{i} \\in \\Bbb{R}^{N} $, $ \\exists \\mathbf{n} $ such that $y_{i}(\\mathbf{x}_{i} \\cdot \\mathbf{n} - c) > 0 $. If our hyperplane intersects the origin, then c = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 3: The Perceptron Algorithm.** \n",
    "\n",
    "Lemma 1 means that we can assume without loss of generality that our dataset is divided by an origin-intersecting hyperplane.\n",
    "\n",
    "$ k \\leftarrow 1; \\mathbf{w}^{0} \\leftarrow \\mathbf{0} $.  \n",
    "While there exists $i \\in \\{1,2,\\dots, n\\}$ such that $y_{i}(\\mathbf{w}^{k} \\cdot \\mathbf{x}_{i}) \\le 0$:  \n",
    "$\\dots$Pick an arbitrary $j \\in \\{1,2,\\dots,n\\}$ such that $y_{j}(\\mathbf{w}^{k} \\cdot \\mathbf{x}_{j}) \\le 0$.  \n",
    "$\\dots \\mathbf{w}^{k+1} \\leftarrow \\mathbf{w}^{k} + y_{j}x_{j}$.  \n",
    "$\\dots k \\leftarrow k + 1$.  \n",
    "Return $\\mathbf{w}^{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**. *For a linearly separable dataset, the Perceptron Algorithm will converge.*\n",
    "\n",
    "**Proof**.\n",
    "\n",
    "Lemma 1 means that we can assume without loss of generality that our dataset is divided by an origin-intersecting hyperplane.\n",
    "\n",
    "In such a case it is trivial to see that we can choose a normal $\\mathbf{n}$ for our hyperplane such that $\\|\\mathbf{n}\\| = 1$.\n",
    "\n",
    "Given a finite dataset, we must have some point lying closest to the separating hyperplane, such that it minimizes $$ y_{i}(\\mathbf{x}_{i} \\cdot \\mathbf{n} ) = 2 \\gamma >  \\gamma \\in \\Bbb{R} \\tag{1} $$\n",
    "\n",
    "We must also have some point furthest from the origin, such that it maximizes $ \\| \\mathbf{x}_{i} \\| = \\frac{1}{2} R < R \\in \\Bbb{R}$\n",
    "\n",
    "We write $\\mathbf{w}^k$ for the k-th iteration of $\\mathbf{w}$.\n",
    "\n",
    "Then $$\n",
    "\\begin{align}\n",
    "\\mathbf{w}^{k+1} \\cdot \\mathbf{n} &= (\\mathbf{w}^k + y_{j}\\mathbf{x}_{j}) \\cdot \\mathbf{n}  \n",
    "\\\\ &= \\mathbf{w}^k \\cdot \\mathbf{n} + y_{j}(\\mathbf{x}_j \\cdot \\mathbf{n}) \\tag{2}\n",
    "\\\\ &> \\mathbf{w}^k \\cdot \\mathbf{n} + \\gamma \\tag{3}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Def.3 we know that $$\\mathbf{w}_0 = \\mathbf{0}.$$\n",
    "Substituting k=0 into (3) gives us $\\mathbf{w}^1 > \\gamma \\tag{4}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting k=1 into (3), and then using {4} gives us  $$\n",
    "\\begin{align}\n",
    "\\mathbf{w}^{2} \\cdot \\mathbf{n} &> \\mathbf{w}^1 \\cdot \\mathbf{n} + \\gamma > \\gamma + \\gamma = 2 \\gamma\\tag{5}\n",
    "\\end{align}$$\n",
    "\n",
    "So by induction $$ \\mathbf{w}^k > k\\gamma \\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (3) \n",
    "> Implement a binary perceptron. (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vector():\n",
    "    \"\"\"Define vector and vector-functions without using NumPy.\"\"\"\n",
    "    \n",
    "    def __init__(self, list_of_floats):\n",
    "        self._ = list_of_floats\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self._)\n",
    "    \n",
    "    def checks(self, vector):\n",
    "        try:\n",
    "            v1 = self._\n",
    "            v2 = vector._\n",
    "        except:\n",
    "            print(f'ERROR: {vector} must be of class \"vector\".')\n",
    "            return 'error'\n",
    "\n",
    "        if len(v2) != len(self._):\n",
    "            print('ERROR: Both vectors must be of same length.')\n",
    "            return 'error'   \n",
    "    \n",
    "    def dot(self, vector):\n",
    "        if self.checks(vector) == 'error':\n",
    "            return\n",
    "                \n",
    "        v1, v2 = self._ , vector._\n",
    "        dot_product = 0.0\n",
    "        for i, _ in enumerate(self._):\n",
    "            dot_product += v1[i] * v2[i]\n",
    "        return dot_product\n",
    "    \n",
    "    def norm(self):\n",
    "        return math.sqrt(self.dot(self))\n",
    "    \n",
    "    def cosine_similarity(self,vector):\n",
    "        cos_theta = self.dot(vector) / (self.norm() * vector.norm())\n",
    "        return cos_theta\n",
    "        \n",
    "    def angle(self,vector):\n",
    "        theta = math.acos(self.cosine_similarity(vector))\n",
    "        return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        v1 (np.array): first vector\n",
    "        v2 (np.array): second vector\"\"\"\n",
    "    \n",
    "    score = numpy.dot(v1,v2) / (numpy.linalg.norm(v1) * numpy.linalg.norm(v2))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, with_print=False):\n",
    "    \"\"\"\n",
    "    Read in labelled data from file.\n",
    "    \n",
    "    Args:\n",
    "        filename (str): Name of file in local directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # open the file\n",
    "    with open(filename,'r') as f:\n",
    "        file_data = f.read()\n",
    "\n",
    "    # split lines\n",
    "    split_data = file_data.split('\\n')\n",
    "\n",
    "    # creat dict to store data\n",
    "    data = {}\n",
    "    for i, datum in enumerate(split_data):\n",
    "        try:\n",
    "            # split the data-vector from the class-label\n",
    "            split = datum.split(',class-')\n",
    "            label = split[1]\n",
    "            \n",
    "            # split the elements of the data-vector\n",
    "            list_of_strings = split[0].split(',')\n",
    "            list_vector = []\n",
    "            \n",
    "            # convert the elements of the data-vector...\n",
    "            # ... from text strings to floating-point numbers\n",
    "            for string in list_of_strings:\n",
    "                element = float(string)\n",
    "                list_vector.append(element)\n",
    "            \n",
    "            # convert the list of floats to a numpy array vector\n",
    "            vector = np.array(list_vector)\n",
    "            \n",
    "            # load the label and vector into the data dict\n",
    "            data[i] = (label, vector)\n",
    "            \n",
    "            if with_print==True:\n",
    "                print(f'Extracted \"{datum}\" to \"{data[i]}\".')\n",
    "        except IndexError:\n",
    "            if with_print==True:\n",
    "                print(f'Could not split \"{datum}\".\\nProbably this was the end of the file.')\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_data('test.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_data('train.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(training_dataset, positive_label, negative_label):\n",
    "    \"\"\"Train Perceptron on given training dataset.\n",
    "    \n",
    "    Args:\n",
    "        training_dataset(dict): Dict with integer keys containing tuples \n",
    "                            with labels and np.array data vectors.\"\"\"\n",
    "    \n",
    "    # load training_dataset\n",
    "    data = training_dataset\n",
    "    \n",
    "    # find length of dataset vectors\n",
    "    vector_length = len(data[0][1])\n",
    "    \n",
    "    # initialize weight_vector\n",
    "    weight_vector = np.zeros(vector_length + 1)\n",
    "    \n",
    "    for datum in data:\n",
    "        label = datum[0]\n",
    "        vector = datum[1]\n",
    "        \n",
    "        if cosine_similarity(weight_vector, vector) >= 0:\n",
    "            perceptron_label = positive_label\n",
    "        else:\n",
    "            perceptron_label = negative_label\n",
    "\n",
    "        if perceptron_label == label:\n",
    "            # correctly classified!\n",
    "            pass\n",
    "        else:\n",
    "            # adjust weights and bias\n",
    "\n",
    "    return vector_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_perceptron(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1', array([5.1, 3.5, 1.4, 0.2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.33"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (4) \n",
    "> Use the binary perceptron to train classifiers to discriminate between (a) class 1 and class 2, (b) class 2 and class 3 and (c) class 1 and class 3. Report the train and test classification accuracies for each of the three classifiers after 20 iterations. Which pair of classes is most difficult to separate? (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (5) \n",
    "> For the classifier (a) implemented in part (3) above, which feature is the most discriminative? (5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (6) \n",
    "> Extend the binary perceptron that you implemented in part (2) above to perform multi-class classification using the 1-vs-rest approach. Report the train and test classification accuracies for each of the three classes after training for 20 iterations. (15 marks),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task (7) \n",
    "> Add an $\\ell_{2}$ regularisation term to your multi-class classifier implemented in question (5). Set the regularisation coefficient to 0.01, 0.1, 1.0, 10.0, 100.0 and compare the train and test classification accuracy for each of the three classes. (10 marks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
