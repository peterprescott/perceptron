{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP527 Data Mining & Visualization: Text Classification Using Binary Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Student 201442927. University of Liverpool.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Explain...\n",
    ">Explain the Perceptron algorithm for the binary classification case, providing its pseudo code. (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Perceptron Algorithm*, published by Frank Rosenblatt in 1958, is inspired by the idea of a biological neuron which is sensitive to a number of stimuli and is deterministically activated when the effect of those combined stimuli exceeds some activation threshold.\n",
    "\n",
    "Mathematically, we model this as the dot product of a vector of quantified stimuli $\\mathbf{x}$ and a vector of weighted sensitivities $\\mathbf{w}$, plus a bias term $ b $.\n",
    "\n",
    "We iterate through our training dataset of vectors with labels ( $\\mathbf{x}_{i} \\in \\Bbb{R}^{N}, y_{i} \\in $ {-1, 1} ), and whenever we get the wrong result, we adjust our weight vector accordingly: if our result was negative when it should have been positive, we *add* the incorrectly classified vector to the weight vector; if it was positive when it should have been negative then we *subtract* the incorrectly classified vector. \n",
    "\n",
    "We also adjust our bias term, by adding the label $y_{i} \\in $ {-1, 1}.\n",
    "\n",
    "We repeat until the Perceptron is able to correctly classify every element in the training set, or when some specified maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the pseudo-code given by [Daume III (2017:43)](http://ciml.info/dl/v0_99/ciml-v0_99-ch04.pdf), we have:\n",
    "\n",
    "---\n",
    "PerceptronTrain($\\mathbf{D}$, *MaxIter*)\n",
    "\n",
    "$w_{d} \\leftarrow 0$, for all d = 1 ... D\n",
    "$b \\leftarrow 0$\n",
    "\n",
    "**for** *iter* = 1 $...$ *MaxIter* **do**  \n",
    "$...$ **for all** ($\\mathbf{x}, y \\in \\mathbf{D})$ **do**  \n",
    "$...$ $...$ $a \\leftarrow \\sum_{d=1}^D w_{d} x_{d} + b$  \n",
    "$...$ $...$ **if** $ya \\le 0$ **then**  \n",
    "$...$ $...$ $...$ $w_{d} \\leftarrow w_{d} + yx_{d}$, for all $d = 1 ... D$  \n",
    "$...$ $...$ $...$ $ b \\leftarrow b + y$  \n",
    "$...$ $...$ **end if**  \n",
    "$...$ **end for**  \n",
    "**end for**  \n",
    "**return** $w_{0}, w_{1}, ..., w_{D}, b  $\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Prove...\n",
    "> Prove that for a linearly separable dataset, the perceptron algorithm will converge. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a hyperplane, and show that any hyperplane $\\in \\Bbb{R}^N$ not intersecting the origin can be mapped to a hyperplane $\\in \\Bbb{R}^{N+1}$ which does intersect the origin; we then give define *linearly separable* in terms of a hyperplane; and we define the *perceptron algorithm* for an origin-intersecting hyperplane. We then prove the convergence of the perceptron algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 1: A Hyperplane.** A *hyperplane* $ H^{N-1} $ is an (N - 1) dimensional subspace of an N dimensional space, defined by a normal $ \\mathbf{n} \\in \\Bbb{R}^{N} $, and some constant $c \\in \\Bbb{R} $, such that $ H^{N-1}  = \\{  \\mathbf{x} \\in \\Bbb{R}^{N} : \\mathbf{x} \\cdot \\mathbf{n} = c \\in \\Bbb{R} \\}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 1**. We note that any hyperplane of dimension (N-1)\n",
    "can be projected to an origin-intersecting hyperplane \n",
    "$ H_{0}^{N} $ of dimension N (within an N+1 dimensional space $ \\Bbb{R}^{N+1} $), \n",
    "by mapping $f(x_{1}, \\dots, x_{N}) \\to (1, x_{1}, \\dots, x_{N})$, \n",
    "and $g(n_{1}, \\dots, n_{N}) \\to (-c, n_{1}, \\dots, n_{N})$ \n",
    "so that if we say $f(\\mathbf{x}) = \\mathbf{x}^\\prime$ \n",
    "and $g(\\mathbf{n}) = \\mathbf{n}^\\prime$ \n",
    "we have $\\mathbf{x}^\\prime \\cdot \\mathbf{n}^\\prime = -c + \\mathbf{x} \\cdot \\mathbf{n} = 0$\n",
    "\n",
    "Therefore we can describe $ H^{N}  = \\{  \\mathbf{x}^\\prime \\in \\Bbb{R}^{N+1} : \\mathbf{x}^\\prime \\cdot \\mathbf{n}^\\prime = 0 \\in \\Bbb{R}  \\}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 2.** A vectorised dataset in an N-dimensional attribute-space is *linearly separable* if there exists a hyperplane $ H $ such that all the points to one side of the hyperplane are of one category, and all the points to the other side are of the other. \n",
    "\n",
    "Thus, given labels $ y_{i} \\in \\{ 1, -1 \\} $ for each datapoint $ \\mathbf{x}_{i} \\in \\Bbb{R}^{N} $, $ \\exists \\mathbf{n} $ such that $\\forall \\mathbf{x}_{i}, \\\\ y_{i}(\\mathbf{x}_{i} \\cdot \\mathbf{n}) - c > 0 $.  \n",
    "If our hyperplane intersects the origin, then c = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition 3: The Perceptron Algorithm.** \n",
    "\n",
    "Since Lemma 1 allows us to assume without loss of generality that our dataset is divided by an origin-intersecting hyperplane, we express the algorithm given that assumption.\n",
    "\n",
    "$ k \\leftarrow 1; \\mathbf{w}^{0} \\leftarrow \\mathbf{0} $.  \n",
    "While $\\exists i \\in \\{1,2,\\dots, n\\}$ such that $y_{i}(\\mathbf{w}^{k} \\cdot \\mathbf{x}_{i}) \\le 0$:  \n",
    "$\\dots$Find $j \\in \\{1,2,\\dots,n\\}$ such that $y_{j}(\\mathbf{w}^{k} \\cdot \\mathbf{x}_{j}) \\le 0$.  \n",
    "$\\dots \\mathbf{w}^{k+1} \\leftarrow \\mathbf{w}^{k} + y_{j}\\mathbf{x}_{j}$.  \n",
    "$\\dots k \\leftarrow k + 1$.  \n",
    "Return $\\mathbf{w}^{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem**. *For a linearly separable dataset, the Perceptron Algorithm will converge.*\n",
    "\n",
    "**Proof**.\n",
    "\n",
    "Lemma 1 means that we can assume without loss of generality that our dataset is divided by an origin-intersecting hyperplane.\n",
    "\n",
    "In such a case it is simple to see that we can choose a normal $\\mathbf{n}$ for our hyperplane such that $\\|\\mathbf{n}\\| = 1 \\tag{0}$.\n",
    "\n",
    "Given a finite dataset, we must have some point lying closest to the separating hyperplane, such that it minimizes $$ y_{i}(\\mathbf{x}_{i} \\cdot \\mathbf{n} ) = 2 \\epsilon >  \\epsilon \\in \\Bbb{R} \\tag{1} $$\n",
    "\n",
    "We must also have some point furthest from the origin, such that it maximizes $$ \\| \\mathbf{x}_{i} \\| = \\frac{1}{2} R < R \\in \\Bbb{R} \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write $\\mathbf{w}^k$ for the k-th iteration of $\\mathbf{w}$.\n",
    "\n",
    "Then the definition of the Perceptron Algorithm, plus (1) gives us$$\n",
    "\\begin{align}\n",
    "\\mathbf{w}^{k+1} \\cdot \\mathbf{n} &= (\\mathbf{w}^k + y_{j}\\mathbf{x}_{j}) \\cdot \\mathbf{n}  \n",
    "\\\\ &= \\mathbf{w}^k \\cdot \\mathbf{n} + y_{j}(\\mathbf{x}_j \\cdot \\mathbf{n})\n",
    "\\\\ &> \\mathbf{w}^k \\cdot \\mathbf{n} + \\epsilon \\tag{3}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $$ \\mathbf{w}^k  \\cdot \\mathbf{n} > k\\epsilon \\tag{4}$$ \n",
    "for k = 1, then it does $\\forall k \\in \\Bbb{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since (3) + (4)   $$\\begin{align} \\implies \\mathbf{w}^{k+1} \\cdot \\mathbf{n} &> \\mathbf{w}^k \\cdot \\mathbf{n} + \\epsilon \\\\ &> k\\epsilon + \\epsilon = (k+1)\\epsilon \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and from Definition 3 we know that $$\\mathbf{w}_0 = \\mathbf{0}.$$\n",
    "so substituting k=0 into (3), with (4) gives us $$\\mathbf{w}^1 > \\mathbf{0} + \\epsilon$$ and thus proving (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have chosen $\\|\\mathbf{n}\\| = 1 $ (0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we know that $$\\|\\mathbf{x} \\| \\| \\mathbf{y} \\| \\ge \\mathbf{x}  \\cdot \\mathbf{y} \\tag{by Cauchy-Schwarz} $$ we therefore have $$\\begin{align} \\|\\mathbf{w}^k \\| = \\|\\mathbf{w}^k \\| \\| \\mathbf{n} \\| &\\ge \\mathbf{w}^k  \\cdot \\mathbf{n}\\\\ &> k\\epsilon \\tag{5} \\end{align}$$\n",
    "which gives us a lower bound on $\\|\\mathbf{w}\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an upper bound, we know from Definition 3 that $ \\mathbf{w}^{k+1} = \\mathbf{w}^{k} + y_{j}\\mathbf{x}_{j}$ and so \n",
    "$$\\begin{align}\\|\\mathbf{w}^{k+1}\\|^2 &= \\|\\mathbf{w}^{k} + y_{j}\\mathbf{x}_{j}\\|^2 \n",
    "\\\\ &=  \\|\\mathbf{w}^{k}\\|^2 + \\|\\mathbf{x}_{j}\\|^2 + 2y_{j}(\\mathbf{w}^k \\cdot \\mathbf{x}_j)\n",
    "\\\\ & \\le \\|\\mathbf{w}^{k}\\|^2 + \\|\\mathbf{x}_{j}\\|^2 \\tag{6}\n",
    "\\end{align}$$\n",
    "since we only update $\\mathbf{w}^k$ when it misclassifies $\\mathbf{x}$ and so $$ y_{j}(\\mathbf{w}^k \\cdot \\mathbf{x}_j) \\le 0 \\tag{Definition 3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting together (6) and (2) gives us $$ \\|\\mathbf{w}^{k+1}\\|^2 \\le \\|\\mathbf{w}^{k}\\|^2 + R^2 \\tag{7}$$\n",
    "and so by induction $$ \\|\\mathbf{w}^{k+1}\\|^2 \\le kR^2 \\tag{8}$$\n",
    "$\\forall k \\in \\Bbb{N}$ since if true for any $k \\in \\Bbb{N}$  \n",
    "then (8) + (7) $\\implies \\|\\mathbf{w}^{k+1}\\|^2 \\le  (k-1)R^2 + R^2 = kR^2 $\n",
    "and Def.3 $\\implies \\mathbf{x}^0 = \\mathbf{0} \\le R^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting together (8) and (5) we then have $$\\begin{align} kR^2 &\\ge \\|\\mathbf{w}^{k+1}\\|^2 \\\\ &> ((k+1)\\epsilon)^2 \\\\&\\ge (k\\epsilon)^2 \\tag{9} \\\\&\\ge k^2\\epsilon^2 \\tag{10}\\end{align}$$\n",
    "\n",
    "(9) is true, since $\\epsilon \\ge 0$ and $k \\in \\Bbb{N}$.\n",
    "\n",
    "From (10), we then have a limit on k: $$ k \\le \\frac{R^2}{\\epsilon^2} $$ and we are done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Implement...\n",
    "> Implement a binary perceptron. (20 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first implement a Dataset object that will load the given data from the `train.data` and `test.data` text-files. We then implement a Perceptron object with Perceptron.train() and Perceptron.test() functions. To avoid having to also implement a Vector object we simply import NumPy. \n",
    "\n",
    "All our final code is viewable in our `perceptron.py` file. Here we simply show the initial iteration of the Perceptron object class for the binary case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from perceptron import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, positive_label='1', negative_label=False,):\n",
    "        \"\"\"Initialize Perceptron with given labels.\n",
    "        \n",
    "        Args:\n",
    "            positive_label (str): Class names to give label `+1`\n",
    "                Default to Class '1'.\n",
    "            negative_label (str): Class names to give label `-1`\n",
    "                Default to Class '2'.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.positive_label = positive_label\n",
    "        self.negative_label = negative_label\n",
    "    \n",
    "    def _label(self, D):\n",
    "        \"\"\"Label elements of dataset D, given Perceptron's positive and negative labels.\n",
    "        \n",
    "        Args:\n",
    "            D (Dataset): Dataset in the form of our declared Dataset class.\n",
    "        \"\"\"\n",
    "        \n",
    "        # store labels in dict\n",
    "        y = {}\n",
    "        for i in range(D.size):\n",
    "            class_name = D.data[i]['class_name']\n",
    "            \n",
    "            if class_name == self.positive_label:\n",
    "                y[i] = 1\n",
    "            \n",
    "            elif class_name == self.negative_label:\n",
    "                y[i] = -1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "                \n",
    "        return y\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        max_iterations = 10000, \n",
    "        max_updates = 100000, \n",
    "        D = Dataset('train.data'),\n",
    "        ):\n",
    "        \"\"\"Train Perceptron for given number of iterations or updates.\n",
    "        \n",
    "        Args:\n",
    "            max_iterations (int): \n",
    "                Maximum number of times to iterate through dataset.\n",
    "                Defaults to ten thousand.\n",
    "            max_updates (int): Max times to update Perceptron weights.\n",
    "                Defaults to one hundred thousand.\n",
    "            D (Dataset): training dataset.\n",
    "                Defaults to loading new Dataset from 'train.data' file.\n",
    "        \"\"\"\n",
    "        \n",
    "        # store each iteration of weights in dict\n",
    "        w = {}\n",
    "            \n",
    "        # initialize weight_vector\n",
    "        w[0] = np.zeros(D.features + 1)\n",
    "        \n",
    "        # get labels for D dataset\n",
    "        y = self._label(D)\n",
    "        \n",
    "        k = 0\n",
    "        updates_after_iteration = {}\n",
    "        for n in range(max_iterations):\n",
    "            \n",
    "            for i in range(D.size):\n",
    "            \n",
    "                x = D.data[i]['x']\n",
    "\n",
    "                if y[i]==0:\n",
    "                    pass\n",
    "                \n",
    "                elif y[i]*(w[k].dot(x)) <= 0:\n",
    "                    # ie. if incorrectly classified\n",
    "                    w[k+1] = w[k] + y[i]*x\n",
    "                    k = k+1\n",
    "                    if k == max_updates:\n",
    "                        break\n",
    "            \n",
    "            updates_after_iteration[n] = k        \n",
    "            if n>0 and updates_after_iteration[n] == updates_after_iteration[n-1]:\n",
    "                break\n",
    "            if k >= max_updates:\n",
    "                break\n",
    "        \n",
    "        self.weights = w[k]\n",
    "        self.updates = k\n",
    "        self.history = w\n",
    "        \n",
    "    def test(self, D = Dataset('test.data'), silence=False):\n",
    "        \"\"\"Test given Perceptron on given Test Dataset and return score.\n",
    "        \n",
    "        Args:\n",
    "            D (Dataset)\n",
    "            silence (Boolean): If 'True' then will not print score.\n",
    "        \"\"\"\n",
    "        \n",
    "        w = self.weights\n",
    "        \n",
    "        # get labels for test dataset\n",
    "        y = self._label(D)\n",
    "     \n",
    "        right = 0\n",
    "        wrong = 0\n",
    "        succeeds = []\n",
    "        fails = []\n",
    "        \n",
    "        for i in range(D.size):\n",
    "            \n",
    "            x = D.data[i]['x']\n",
    "            if y[i] == 0:\n",
    "                pass\n",
    "            elif y[i] * w.dot(x) > 0:\n",
    "                right += 1\n",
    "                succeeds.append((i, x, y[i]))\n",
    "            else:\n",
    "                wrong += 1\n",
    "                fails.append((i, x, D.data[i]['class_name']))\n",
    "        \n",
    "        self.succeeds = succeeds\n",
    "        self.fails = fails\n",
    "        \n",
    "        self.score = right/(right+wrong) * 100\n",
    "        if silence == False:\n",
    "            print(f'Success rate: {self.score}%')\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4. Train...\n",
    "> Use the binary perceptron to train classifiers to discriminate between (a) class 1 and class 2, (b) class 2 and class 3 and (c) class 1 and class 3. Report the train and test classification accuracies for each of the three classifiers after 20 iterations. Which pair of classes is most difficult to separate? (20 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "a = Perceptron('1','2')\n",
    "a.train(max_iterations=20)\n",
    "a.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 50.0%\n"
     ]
    }
   ],
   "source": [
    "b = Perceptron('2','3')\n",
    "b.train(max_iterations=20)\n",
    "b.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "c = Perceptron('1','3')\n",
    "c.train(max_iterations=20)\n",
    "c.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes '2' and '3' seem the most difficult to separate, as they have the worst test score after training with 20 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a) converged after 5 updates.\n",
      "(c) converged after 5 updates.\n"
     ]
    }
   ],
   "source": [
    "print(f'(a) converged after {a.updates} updates.')\n",
    "print(f'(c) converged after {c.updates} updates.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Perceptron Algorithm managed to converge on a solution to dividing classes '1' and '2' and classes '1' and '3' after just five updates each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 93.75%\n"
     ]
    }
   ],
   "source": [
    "b.train(max_iterations=100000)\n",
    "b.test(D=Dataset('train.data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, even if we allow our Perception Algorithm 100,000 iterations, it will still not converge on a weight-solution that gives 100% accuracy on our training dataset -- this suggests it is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_failure(updates=10000):\n",
    "    score = {}\n",
    "    for i in range(updates):\n",
    "        d = Perceptron('2','3')\n",
    "        d.train(max_updates=i)\n",
    "        d.test(D=Dataset('train.data'), silence=True)\n",
    "        score[i] = len(d.fails)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = minimize_failure(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(374, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import operator\n",
    "min(scores.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(60, array([1. , 5.9, 3.2, 4.8, 1.8]), '2'),\n",
       " (73, array([1. , 6. , 2.7, 5.1, 1.6]), '2')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize = Perceptron('2','3')\n",
    "minimize.train(max_updates=374)\n",
    "minimize.test(D=Dataset('train.data'),silence=True)\n",
    "minimize.fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further analysis suggests that is the 60th and 73rd entries (starting from zero) of our dataset that are responsible for the classes not being linearly separable. This suggests they may be incorrectly classified, and would explain why the Perceptron Algorithm is unable to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5. Which...\n",
    "> For the classifier (a) implemented in part (3) above, which feature is the most discriminative? (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1. ,  1.3,  4.1, -5.2, -2.2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have included the bias term as the first (or perhaps *zero-th*) entry in our weight vector, the fourth and largest (in terms of absolute magnitude) value, that is `-5.2`, corresponds to **the third feature** of the dataset, and therefore seems to be the most discriminative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6. Extend...\n",
    "> Extend the binary perceptron that you implemented in part (2) above to perform multi-class classification using the 1-vs-rest approach. Report the train and test classification accuracies for each of the three classes after training for 20 iterations. (15 marks),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend the binary perceptron we simply set the default value of `negative_value` to `False`, and adjusted the labelling function of our Perceptron class so that if no negative_value is explicitly given, then to label negatively all datapoints whose class is not the same as `positive_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, positive_label='1', negative_label=False):\n",
    "        self.positive_label = positive_label\n",
    "        self.negative_label = negative_label\n",
    "    \n",
    "    def _label(self, D):\n",
    "        \"\"\"Label elements of dataset D, given Perceptron's positive and negative labels.\n",
    "        \n",
    "        Args:\n",
    "            D (Dataset): Dataset in the form of our declared Dataset class.\n",
    "        \"\"\"\n",
    "        \n",
    "        # store labels in dict\n",
    "        y = {}\n",
    "        for i in range(D.size):\n",
    "            class_name = D.data[i]['class_name']\n",
    "            \n",
    "            if class_name == self.positive_label:\n",
    "                y[i] = 1\n",
    "            elif self.negative_label:\n",
    "                if class_name == self.negative_label:\n",
    "                    y[i] = -1\n",
    "                else:\n",
    "                    y[i] = 0\n",
    "            else:\n",
    "                y[i] = -1\n",
    "                \n",
    "        return y\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate we import our adapted Perceptron from `perceptron.py` and report the accuracy for each class after training for up to 20 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptron import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 100.0%\n"
     ]
    }
   ],
   "source": [
    "d = Perceptron('1')\n",
    "d.train(max_iterations=20)\n",
    "d.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 66.66666666666666%\n"
     ]
    }
   ],
   "source": [
    "e = Perceptron('2')\n",
    "e.train(max_iterations=20)\n",
    "e.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success rate: 70.0%\n"
     ]
    }
   ],
   "source": [
    "f = Perceptron('3')\n",
    "f.train(max_iterations=20)\n",
    "f.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7. Regularise...\n",
    "> Add an $\\ell_{2}$ regularisation term to your multi-class classifier implemented in question (5).  \n",
    "> Set the regularisation coefficient to 0.01, 0.1, 1.0, 10.0, 100.0 and compare the train and test classification accuracy for each of the three classes. (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add an $\\ell_{2}$ regularisation term we simply adjusted the update rule of our Perceptron.train() function, and added a `regularisation_coefficient` parameter so that we can set it as we call it. We set the default to be zero, so that if we don't specify it our Perceptron algorithm will function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new update rule\n",
    "w[k+1] = w[k] + y[i]*x - 2 * regularisation_coefficient * w[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the updated Perceptron\n",
    "from perceptron import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "classes = ['1','2','3']\n",
    "max_iterations = [20, 100, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularize = {}\n",
    "machine = {}\n",
    "\n",
    "for c in classes:\n",
    "    machine[c] = Perceptron(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coef in coefs:\n",
    "    regularize[coef] = {}\n",
    "    \n",
    "    for c in classes:\n",
    "        regularize[coef][c] = {}\n",
    "        for i in max_iterations:\n",
    "            machine[c].train(regularisation_coefficient = coef, max_iterations = i)\n",
    "            machine[c].test(silence=True)\n",
    "            regularize[coef][c][i] = machine[c].score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the accuracy scores for each class, with the regularisation coefficient to 0.01, 0.1, 1.0, 10.0, 100.0, and with max_iterations of 20, 100 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularisation Coefficient: 0.01\n",
      "\n",
      "Class 1:\n",
      "Iterations: 20. Score: 100.0\n",
      "Iterations: 100. Score: 100.0\n",
      "Iterations: 1000. Score: 100.0\n",
      "\n",
      "\n",
      "Class 2:\n",
      "Iterations: 20. Score: 66.66666666666666\n",
      "Iterations: 100. Score: 60.0\n",
      "Iterations: 1000. Score: 66.66666666666666\n",
      "\n",
      "\n",
      "Class 3:\n",
      "Iterations: 20. Score: 66.66666666666666\n",
      "Iterations: 100. Score: 70.0\n",
      "Iterations: 1000. Score: 66.66666666666666\n",
      "\n",
      "\n",
      "Regularisation Coefficient: 0.1\n",
      "\n",
      "Class 1:\n",
      "Iterations: 20. Score: 100.0\n",
      "Iterations: 100. Score: 100.0\n",
      "Iterations: 1000. Score: 100.0\n",
      "\n",
      "\n",
      "Class 2:\n",
      "Iterations: 20. Score: 66.66666666666666\n",
      "Iterations: 100. Score: 66.66666666666666\n",
      "Iterations: 1000. Score: 66.66666666666666\n",
      "\n",
      "\n",
      "Class 3:\n",
      "Iterations: 20. Score: 33.33333333333333\n",
      "Iterations: 100. Score: 53.333333333333336\n",
      "Iterations: 1000. Score: 53.333333333333336\n",
      "\n",
      "\n",
      "Regularisation Coefficient: 1.0\n",
      "\n",
      "Class 1:\n",
      "Iterations: 20. Score: 66.66666666666666\n",
      "Iterations: 100. Score: 66.66666666666666\n",
      "Iterations: 1000. Score: 66.66666666666666\n",
      "\n",
      "\n",
      "Class 2:\n",
      "Iterations: 20. Score: 66.66666666666666\n",
      "Iterations: 100. Score: 66.66666666666666\n",
      "Iterations: 1000. Score: 66.66666666666666\n",
      "\n",
      "\n",
      "Class 3:\n",
      "Iterations: 20. Score: 33.33333333333333\n",
      "Iterations: 100. Score: 33.33333333333333\n",
      "Iterations: 1000. Score: 33.33333333333333\n",
      "\n",
      "\n",
      "Regularisation Coefficient: 10.0\n",
      "\n",
      "Class 1:\n",
      "Iterations: 20. Score: 66.66666666666666\n",
      "Iterations: 100. Score: 66.66666666666666\n",
      "Iterations: 1000. Score: 0.0\n",
      "\n",
      "\n",
      "Class 2:\n",
      "Iterations: 20. Score: 66.66666666666666\n",
      "Iterations: 100. Score: 66.66666666666666\n",
      "Iterations: 1000. Score: 0.0\n",
      "\n",
      "\n",
      "Class 3:\n",
      "Iterations: 20. Score: 33.33333333333333\n",
      "Iterations: 100. Score: 33.33333333333333\n",
      "Iterations: 1000. Score: 0.0\n",
      "\n",
      "\n",
      "Regularisation Coefficient: 100.0\n",
      "\n",
      "Class 1:\n",
      "Iterations: 20. Score: 66.66666666666666\n",
      "Iterations: 100. Score: 0.0\n",
      "Iterations: 1000. Score: 0.0\n",
      "\n",
      "\n",
      "Class 2:\n",
      "Iterations: 20. Score: 66.66666666666666\n",
      "Iterations: 100. Score: 0.0\n",
      "Iterations: 1000. Score: 0.0\n",
      "\n",
      "\n",
      "Class 3:\n",
      "Iterations: 20. Score: 33.33333333333333\n",
      "Iterations: 100. Score: 0.0\n",
      "Iterations: 1000. Score: 0.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for coef in coefs:\n",
    "    print(f'Regularisation Coefficient: {coef}\\n')\n",
    "    for c in classes:\n",
    "        print(f'Class {c}:')\n",
    "        for i in max_iterations:\n",
    "            print(f'Iterations: {i}. Score: {regularize[coef][c][i]}')\n",
    "        print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
